{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code Accessing Data from Apache Hive\n",
    "# sample 1\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "from impala.dbapi import connect\n",
    "from impala.util import as_pandas\n",
    "\n",
    "# Specify HIVE_HMS_HOST as an environment variable in your project settings\n",
    "HIVE_HMS_HOST = os.getenv('HIVE_HS2_HOST', '<hiveserver2_hostname>')\n",
    "\n",
    "# This connection string depends on your cluster setup and authentication mechanism\n",
    "conn = connect(host=HIVE_HS2_HOST,\n",
    "               port='10000',\n",
    "               auth_mechanism='GSSAPI',\n",
    "               kerberos_service_name='hive')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SHOW TABLES')\n",
    "tables = as_pandas(cursor)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 2\n",
    "  hive_cur.execute('SELECT * FROM  `my_db`.`my_first_hdfs_based_external_table` ')\n",
    "  # Fetch ALL table records in the tuple (could be extremely slow for large tables)\n",
    "  records = hive_cur.fetchall()\n",
    "  for record in records:\n",
    "      # Do smth with record\n",
    "      pass\n",
    "  # Fetch ONE working like iterator\n",
    "  record = hive_cur.fetchone()\n",
    "  while record is not None:\n",
    "      # Do smth with record\n",
    "      record = hive_cur.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 3 \n",
    "import pandas as pd\n",
    "from pyhive import hive\n",
    "class HiveConnection:\n",
    "    @staticmethod\n",
    "    def select_query(query_str: str, database:str =HIVE_SCHEMA) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Execute a select query which returns a result set\n",
    "        :param query_str: select query to be executed\n",
    "        :param database: Hive Schema\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        conn = hive.Connection(host=HIVE_URL, port=HIVE_PORT, database=database, username=HIVE_USER)\n",
    "\n",
    "        try:\n",
    "            result = pd.read_sql(query_str, conn)\n",
    "            return result\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def execute_query(query_str: str, database: str=HIVE_SCHEMA):\n",
    "        \"\"\"\n",
    "        execute an query which does not return a result set.\n",
    "        ex: INSERT, CREATE, DROP, ALTER TABLE\n",
    "        :param query_str: Hive query to be executed\n",
    "        :param database: Hive Schema\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        conn = hive.Connection(host=HIVE_URL, port=HIVE_PORT, database=database, username=HIVE_USER)\n",
    "        cur = conn.cursor()\n",
    "        # Make sure to set the staging default to HDFS to avoid some potential S3 related errors\n",
    "        cur.execute(\"SET hive.exec.stagingdir=/tmp/hive/\")\n",
    "        cur.execute(\"SET hive.exec.scratchdir=/tmp/hive/\")\n",
    "        try:\n",
    "            cur.execute(query_str)\n",
    "            return \"SUCCESS\"\n",
    "        finally:\n",
    "            conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
